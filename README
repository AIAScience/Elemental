Elemental
=========

Elemental is a high-level high-performance framework for distributed-memory 
dense linear algebra that is inspired by [PLAPACK][1] and, more recently, 
[FLAME][2].

The key idea behind the project is that an appropriate level of abstraction 
allows one to focus on algorithms rather than getting tied down in 
implementation details.

License
=======
New BSD License

Currently Supported Routines
============================
- Dense level 3 BLAS
- Cholesky factorization
- Gaussian elimination
- Generalized Hermitian eigenvalue problems
- Hermitian eigenvalue problems
- LU factorization
- QR factorization
- UT transform application

Build
=====
The build system is not yet automated. However, all that is currently required
is to define the appropriate MPI compiler and BLAS/LAPACK libraries at the 
top of the Makefile.

There are two main build modes, *debug* and *release*. The former maintains a 
call stack and performs judicious error-checking. Thus debug-mode should be used
when testing a new algorithm. Each of those two build modes further supports 
both pure MPI and hybrid OpenMP/MPI parallelism. You can build the pure MPI 
debug library with:
      make pure-debug
Likewise, the hybrid OpenMP/MPI library can be built with:
      make hybrid-debug
If you would also like to build the debug test drivers for the distributed 
BLAS and LAPACK routines, run either
      make test-pure-debug
or
      make test-hybrid-debug
Similarly, you can build the baremetal versions of the library with
      make pure-release
and
      make hybrid-release
The commands
      make test-pure-release
and
      make test-hybrid-release
perform the equivalent actions as in the debug case.

Alternatively, all four library versions may be built with
      make
and all four versions of test drivers may be built with
      make test

Tuning
======
There are a few categories of parameters that can be tuned for better 
performance:

- The algorithmic blocksize can be adjusted at any time with the function 

  void elemental::SetBlocksize( int blocksize );

- The process grid dimensions should typically be square, but they can be 
  manually chosen to build an r x c grid using the extended process grid 
  constructor:

  elemental::Grid::Grid( MPI_Comm comm, int r, int c );

- The local blocksizes for Hemv and Symv may be set for single-precision, 
  double-precision, single-precision complex, and double-precision complex 
  using:

  void elemental::blas::SetLocalHemvFloatBlocksize( int blocksize ); 
  void elemental::blas::SetLocalHemvDoubleBlocksize( int blocksize );
  void elemental::blas::SetLocalHemvComplexFloatBlocksize( int blocksize );
  void elemental::blas::SetLocalHemvComplexDoubleBlocksize( int blocksize );

  void elemental::blas::SetLocalSymvFloatBlocksize( int blocksize );
  void elemental::blas::SetLocalSymvDoubleBlocksize( int blocksize );
  void elemental::blas::SetLocalSymvComplexFloatBlocksize( int blocksize );
  void elemental::blas::SetLocalSymvComplexDoubleBlocksize( int blocksize );

  These interfaces were exposed because they play an important role in the 
  speed of the parallel Householder tridiagonalization routine, 
  elemental::lapack::Tridiag.

Overview
--------
The project is designed with the knowledge that distribution blocksizes do not 
have to equal algorithmic blocksizes. With this in mind, Elemental distributes 
matrices element by element for performance robustness: unaligned distributions
are *cheaply* and *automatically* fixed via a bijection implemented through 
MPI's SendRecv.

Taking cues from [PLAPACK][1], Elemental does not focus solely on manipulating 
traditional 2d matrix distributions, but is designed to use 1d (or *vector*) 
distributions when appropriate. For instance, when solving a triangular 
system with many right-hand sides, it is extremely efficient to solve with 
each right-hand side owned by a single process.

Since moving between different distributions is a necessary part of 
distributed memory algorithms, we introduce the following notation:

*  If a matrix `A` has each column distributed in a manner denoted by the symbol
   `X`, and each row distributed in a manner denoted by `Y`, then we write 
   `A[X,Y]` to represent the distribution.
*  A typical 2d matrix distribution is written `A[MC,MR]`, where `MC` means 
   "distributed like a *Matrix* *Column*", and `MR` means "distributed like a 
   *Matrix* *Row*."
*  Similarly, `A[MR,MC]` will denote the transposed matrix distribution.
*  `VC` represents a *Vector* distribution generated by a 1d *Column*-major 
   wrapping of the 2d process grid.
*  `VR` represents a *Vector* distribution generated by a 1d *Row*-major 
   wrapping of the 2d process grid.
*  `*` represents no distribution (complete duplication on all processes)
*  Thus `A[*,VC]` represents a matrix `A` where each column is owned by a 
   single process and they have been ordered over the process grid in a 
   column-major fashion.

Basic Example
-------------
Elemental has a templated distributed matrix class, `DistMatrix`, that allows 
for the specification of these distributions. We can create an `m x n` matrix 
in a matrix distribution over MPI_COMM_WORLD with:

    Grid g( MPI_COMM_WORLD );
    DistMatrix<double,MC,MR> A(m,n,g);

Similarly, we can create an `m x n` matrix in a transposed matrix distribution
using

    DistMatrix<double,MR,MC> A(m,n,g);

Redistribution is handled automatically via overloading the `=` operator:

    DistMatrix<double,MR,MC  > A(m,n,g);
    DistMatrix<double,VC,Star> B(m,n,g);
    // Fill matrix A...
    B = A;

[1]: http://www.cs.utexas.edu/users/plapack/new/using.html
[2]: http://z.cs.utexas.edu/wiki/flame.wiki/FrontPage

