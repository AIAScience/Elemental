Elemental
=========

Elemental is a high-level high-performance framework for distributed-memory 
dense linear algebra that is inspired by [PLAPACK][1] and, more recently, 
[FLAME][2].

The key idea behind the project is that an appropriate level of abstraction 
allows one to focus on *algorithms* rather than getting tied down in 
implementation details.

License
=======
GNU Lesser General Public License version 3 (LGPLv3).

Current Status
==============
- All options of Level 3 BLAS (Gemm, Hemm, Her2k, Herk, Symm, Syr2k, Syrk, Trmm,
- The Level 3 BLAS is fully functional, as well as Cholesky and LU 
  factorizations, Gaussian elimination, triangular inversion, and Householder 
  tridiagonalization. Performance is equivalent or superior to ScaLAPACK for 
  all tested routines.

Build
=====
The build system is not yet automated. However, all that is currently required
is to define the appropriate MPI compiler and BLAS/LAPACK libraries at the 
top of the Makefile.

There are two main build modes, *debug* and *release*. The former maintains a 
call stack and performs judicious error-checking. Thus debug-mode should be used
when testing a new algorithm. You can build it with
      make debug
If you would also like to build the debug test drivers for the distributed 
BLAS and LAPACK routines, run
      make test-debug

Similarly, you can build the baremetal version of the library by running 
      make release
The command
      make test-release
perform the equivalent action as in the debug case.

If neither debug nor release modes are specified, both are built. The relevant
commands are
      make
      make test

Overview
--------
The project is designed with the knowledge that distribution blocksizes do not 
have to equal algorithmic blocksizes. With this in mind, Elemental distributes 
matrices element by element for performance robustness: unaligned distributions
are *cheaply* and *automatically* fixed via a bijection implemented through 
MPI's SendRecv.

Taking cues from [PLAPACK][1], Elemental does not focus solely on manipulating 
traditional 2d matrix distributions, but is designed to use 1d (or *vector*) 
distributions when appropriate. For instance, when solving a triangular 
system with many right-hand sides, it is extremely efficient to solve with 
each right-hand side owned by a single process.

Since moving between different distributions is a necessary part of 
distributed memory algorithms, we introduce the following notation:

*  If a matrix `A` has each column distributed in a manner denoted by the symbol
   `X`, and each row distributed in a manner denoted by `Y`, then we write 
   `A[X,Y]` to represent the distribution.
*  A typical 2d matrix distribution is written `A[MC,MR]`, where `MC` means 
   "distributed like a *Matrix* *Column*", and `MR` means "distributed like a 
   *Matrix* *Row*."
*  Similarly, `A[MR,MC]` will denote the transposed matrix distribution.
*  `VC` represents a *Vector* distribution generated by a 1d *Column*-major 
   wrapping of the 2d process grid.
*  `VR` represents a *Vector* distribution generated by a 1d *Row*-major 
   wrapping of the 2d process grid.
*  `*` represents no distribution (complete duplication on all processes)
*  Thus `A[*,VC]` represents a matrix `A` where each column is owned by a 
   single process and they have been ordered over the process grid in a 
   column-major fashion.

Basic Example
-------------
Elemental has a templated distributed matrix class, `DistMatrix`, that allows 
for the specification of these distributions. We can create an `m x n` matrix 
in a matrix distribution over MPI_COMM_WORLD with:

    Grid grid( MPI_COMM_WORLD );
    DistMatrix<double,MC,MR> A(m,n,grid);

Similarly, we can create an `m x n` matrix in a transposed matrix distribution
using

    DistMatrix<double,MR,MC> A(m,n,grid);

Redistribution is handled automatically via overloading the `=` operator:

    DistMatrix<double,MR,MC  > A(m,n,grid);
    DistMatrix<double,VC,Star> B(m,n,grid);
    // Fill matrix A...
    B = A;

Example Driver
--------------
The following example shows how to perform distributed GEMM (GEneral Matrix-
Matrix multiplication) on random matrices:

    #include "Elemental.h"
    using namespace std;
    using namespace Elemental;

    void
    Usage()
    {
        cout << "GEMM Driver:                    " << endl;    
        cout << endl;
        cout << "  mpirun -np <#> ./Gemm <m> <nb>" << endl;
        cout << endl;
        cout << "  <m>: problem size             " << endl;
        cout << "  <nb>: algorithmic blocksize   " << endl;
    }

    int
    main( int argc, char* argv[] )
    {
        int rank;
        Elemental::Init( &argc, &argv );
        MPI_Comm_rank( MPI_COMM_WORLD, &rank );
        if( argc != 3 )
        {
            if( rank == 0 ) 
                Usage();
            Elemental::Finalize();
            return 0;
        }
        int m = atoi(argv[1]);
        int nb = atoi(argv[2]);

        const Grid grid( MPI_COMM_WORLD );
        SetBlocksize( nb );

        DistMatrix<double,MC,MR> A(m,m,grid);
        DistMatrix<double,MC,MR> B(m,m,grid);
        DistMatrix<double,MC,MR> C(m,m,grid);

        A.SetToRandom();
        B.SetToRandom();
        C.SetToRandom();

        A.Print("A");
        B.Print("B");
        C.Print("C");

        // Form C := 2 A B + 3 C
        BLAS::Gemm( Normal, Normal, 2.0, A, B, 3.0, C );

        C.Print("C := 2 A B + 3 C");

        Elemental::Finalize();
        return 0;
    }

Cholesky Factorization Example
------------------------------
The following demonstrates a full templated implementation 
(minus some error-checking) of a left-looking lower Cholesky factorization:

    /*
       Parallelization of Variant 2 Lower Cholesky factorization. 

       Original serial update:
       ------------------------
       A11 := A11 - A10 A10^H
       A11 := Chol(A11)
       A21 := A21 - A20 A10^H
       A21 := A21 tril(A11)^-H
       ------------------------

       Our parallel update:
       -----------------------------------------------------
       A10[* ,MR] <- A10[MC,MR]
       X11[MC,* ] := A10[MC,MR] (A01[* ,MR])^H
       A11[MC,MR] := A11[MC,MR] - (SumRow(X11[MC,* ]))[* ,MR]

       A11[* ,* ] <- A11[MC,MR]   
       A11[* ,* ] := Chol(A11[* ,* ])
       A11[MC,MR] <- A11[* ,* ]

       X21[MC,* ] := A20[MC,MR] (A10[* ,MR])^H
       A21[MC,MR] := A21[MC,MR] - (SumRow(X21[MC,* ]))[* ,MR]

       A21[VC,* ] <- A21[MC,MR]
       A21[VC,* ] := A21[VC,* ] tril(A11[* ,* ])^-H
       A21[MC,MR] <- A21[VC,* ]
       -----------------------------------------------------
    */
    template<typename T>
    void
    Elemental::LAPACK::Internal::CholL_Var2
    ( DistMatrix<T,MC,MR>& A )
    {
        const Grid& grid = A.Grid();

        // Matrix views
        DistMatrix<T,MC,MR> 
            ATL(grid), ATR(grid),  A00(grid), A01(grid), A02(grid),
            ABL(grid), ABR(grid),  A10(grid), A11(grid), A12(grid),
                                   A20(grid), A21(grid), A22(grid);

        // Temporary distributions
        DistMatrix<T,Star,MR  > A10_Star_MR(grid);
        DistMatrix<T,Star,Star> A11_Star_Star(grid);
        DistMatrix<T,VC,  Star> A21_VC_Star(grid);
        DistMatrix<T,MC,  Star> X11_MC_Star(grid);
        DistMatrix<T,MC,  Star> X21_MC_Star(grid);

        // Start the algorithm
        PartitionDownDiagonal( A, ATL, ATR,
                                  ABL, ABR );
        while( ATL.Height() < A.Height() )
        {
            RepartitionDownDiagonal( ATL, /**/ ATR,  A00, /**/ A01, A02,
                                    /*************/ /******************/
                                          /**/       A10, /**/ A11, A12,
                                     ABL, /**/ ABR,  A20, /**/ A21, A22 );

            A10_Star_MR.ConformWith( A10 );
            X11_MC_Star.AlignWith( A10 );
            X11_MC_Star.ResizeTo( A11.Height(), A11.Width() );
            X21_MC_Star.AlignWith( A20 );
            X21_MC_Star.ResizeTo( A21.Height(), A21.Width() );
            //--------------------------------------------------------------------//
            A10_Star_MR = A10;
            BLAS::Gemm( Normal, ConjugateTranspose,
                        (T)1, A10.LockedLocalMatrix(),
                              A10_Star_MR.LockedLocalMatrix(),
                        (T)0, X11_MC_Star.LocalMatrix()       );
            A11.ReduceScatterUpdate( (T)-1, X11_MC_Star );

            A11_Star_Star = A11;
            LAPACK::Chol( Lower, A11_Star_Star.LocalMatrix() );
            A11 = A11_Star_Star;

            BLAS::Gemm( Normal, ConjugateTranspose,
                        (T)1, A20.LockedLocalMatrix(),
                              A10_Star_MR.LockedLocalMatrix(),
                        (T)0, X21_MC_Star.LocalMatrix()       );
            A21.ReduceScatterUpdate( (T)-1, X21_MC_Star );

            A21_VC_Star = A21;
            BLAS::Trsm( Right, Lower, ConjugateTranspose, NonUnit,
                        (T)1, A11_Star_Star.LockedLocalMatrix(),
                              A21_VC_Star.LocalMatrix()           );
            A21 = A21_VC_Star;
            //--------------------------------------------------------------------//
            A10_Star_MR.FreeConstraints();
            X11_MC_Star.FreeConstraints();
            X21_MC_Star.FreeConstraints();

            SlidePartitionDownDiagonal( ATL, /**/ ATR,  A00, A01, /**/ A02,
                                             /**/       A10, A11, /**/ A12,
                                       /*************/ /******************/
                                        ABL, /**/ ABR,  A20, A21, /**/ A22 );
        }
    }

[1]: http://www.cs.utexas.edu/users/plapack/new/using.html
[2]: http://z.cs.utexas.edu/wiki/flame.wiki/FrontPage

