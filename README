Elemental
=========

Elemental is a high-level high-performance framework for distributed-memory 
dense linear algebra that is inspired by [PLAPACK][1] and, more recently, 
[FLAME][2].

The key idea behind the project is that an appropriate level of abstraction 
allows one to focus on algorithms rather than getting tied down in 
implementation details.

License
=======
New BSD License

Currently Supported Routines
============================
- Dense level 3 BLAS
- Cholesky factorization
- Gaussian elimination
- Generalized Hermitian eigenvalue problems
- Hermitian eigenvalue problems
- LU factorization
- QR factorization
- UT transform application

Build
=====
There is now a CMake [http://www.cmake.org] build script which supports building
with and without PMRRR and attempts to search for the appropriate BLAS/LAPACK 
configurations. The four build-modes are:
- HybridDebug
- HybridRelease
- PureDebug
- PureRelease
Each should be built within its own separate build directory. For instance, 
to build the pure-MPI release library with the C++ MPI compiler mpicxx 
and the C MPI compiler mpicc:
      cd elemental
      mkdir -p build/PureRelease
      cd build/PureRelease
      cmake -DCMAKE_BUILD_TYPE=PureRelease \
            -DCMAKE_CXX_COMPILER=`which mpicxx` \
            -DCMAKE_C_COMPILER=`which mpicc` ../..
      make
If the correct BLAS/LAPACK libraries are not found, try specifying them using
by appending the additional cmake flags of the form:
  -DPURE_MATH_LIBS="-L/path/to/first/lib -lfirst_lib etc."
  -DHYBRID_MATH_LIBS="-L/path/to/first/lib -lfirst_lib etc."
where the pure math libs should be optimized for single code usage and the 
hybrid math libs should be threaded if possible.

On BlueGene/P, one should instead run
      cd elemental
      mkdir -p build/PureRelease
      cd build/PureRelease
      cmake -DCMAKE_BUILD_TYPE=PureRelease \
            -DCMAKE_TOOLCHAIN_FILE=../../cmake/Toolchain-BlueGeneP-xl.cmake \
            ../..
      make
The CMake build system is still new, so please email jack.poulson@gmail.com with
any build problems or comments.

Tuning
======
There are a few categories of parameters that can be tuned for better 
performance:

- The algorithmic blocksize can be adjusted at any time with the function 

  void elemental::SetBlocksize( int blocksize );

- The process grid dimensions should typically be square, but they can be 
  manually chosen to build an r x c grid using the extended process grid 
  constructor:

  elemental::Grid::Grid( MPI_Comm comm, int r, int c );

- The Householder tridiagonalization approach may be chosen using the routine:

    void elemental::lapack::internal::SetTridiagApproach
    ( elemental::lapack::internal::TridiagApproach approach );

  The choices are:
    elemental::lapack::internal::TRIDIAG_NORMAL
    elemental::lapack::internal::TRIDIAG_SQUARE
    elemental::lapack::internal::TRIDIAG_DEFAULT
  The first always uses the general nonsquare grid algorithm, even when
  running on a square grid. The second drops down to the largest perfect square
  number of processors less than or equal to the number available in order to
  lower communication costs. However, in order to do so, we must choose how to
  order the perfect square number of processes into a grid, and the choice is
  often important for performance. One can choose between row-major and 
  column-major ordering via the routine

    void elemental::lapack::internal::SetTridiagSquareGridOrder
    ( elemental::lapack::internal::GridOrder order );

  The choices are:
    elemental::lapack::internal::COL_MAJOR
    elemental::lapack::internal::ROW_MAJOR
  Finally, the default option uses the square grid algorithm if the input matrix
  is already distributed over a square grid, otherwise, the normal nonsquare 
  grid algorithm is used.

- The local blocksizes for Hemv and Symv may be set for single-precision, 
  double-precision, single-precision complex, and double-precision complex 
  using:

  void elemental::blas::SetLocalHemvFloatBlocksize( int blocksize ); 
  void elemental::blas::SetLocalHemvDoubleBlocksize( int blocksize );
  void elemental::blas::SetLocalHemvComplexFloatBlocksize( int blocksize );
  void elemental::blas::SetLocalHemvComplexDoubleBlocksize( int blocksize );

  void elemental::blas::SetLocalSymvFloatBlocksize( int blocksize );
  void elemental::blas::SetLocalSymvDoubleBlocksize( int blocksize );
  void elemental::blas::SetLocalSymvComplexFloatBlocksize( int blocksize );
  void elemental::blas::SetLocalSymvComplexDoubleBlocksize( int blocksize );

  These interfaces were exposed because they play an important role in the 
  speed of the parallel Householder tridiagonalization routine, 
  elemental::lapack::Tridiag.

Overview
--------
The project is designed with the knowledge that distribution blocksizes do not 
have to equal algorithmic blocksizes. With this in mind, Elemental distributes 
matrices element by element for performance robustness: unaligned distributions
are *cheaply* and *automatically* fixed via a bijection implemented through 
MPI's SendRecv.

Taking cues from [PLAPACK][1], Elemental does not focus solely on manipulating 
traditional 2d matrix distributions, but is designed to use 1d (or *vector*) 
distributions when appropriate. For instance, when solving a triangular 
system with many right-hand sides, it is extremely efficient to solve with 
each right-hand side owned by a single process.

Since moving between different distributions is a necessary part of 
distributed memory algorithms, we introduce the following notation:

*  If a matrix `A` has each column distributed in a manner denoted by the symbol
   `X`, and each row distributed in a manner denoted by `Y`, then we write 
   `A[X,Y]` to represent the distribution.
*  A typical 2d matrix distribution is written `A[MC,MR]`, where `MC` means 
   "distributed like a *Matrix* *Column*", and `MR` means "distributed like a 
   *Matrix* *Row*."
*  Similarly, `A[MR,MC]` will denote the transposed matrix distribution.
*  `VC` represents a *Vector* distribution generated by a 1d *Column*-major 
   wrapping of the 2d process grid.
*  `VR` represents a *Vector* distribution generated by a 1d *Row*-major 
   wrapping of the 2d process grid.
*  `*` represents no distribution (complete duplication on all processes)
*  Thus `A[*,VC]` represents a matrix `A` where each column is owned by a 
   single process and they have been ordered over the process grid in a 
   column-major fashion.

Basic Example
-------------
Elemental has a templated distributed matrix class, `DistMatrix`, that allows 
for the specification of these distributions. We can create an `m x n` matrix 
in a matrix distribution over MPI_COMM_WORLD with:

    Grid g( MPI_COMM_WORLD );
    DistMatrix<double,MC,MR> A(m,n,g);

Similarly, we can create an `m x n` matrix in a transposed matrix distribution
using

    DistMatrix<double,MR,MC> A(m,n,g);

Redistribution is handled automatically via overloading the `=` operator:

    DistMatrix<double,MR,MC  > A(m,n,g);
    DistMatrix<double,VC,Star> B(m,n,g);
    // Fill matrix A...
    B = A;

[1]: http://www.cs.utexas.edu/users/plapack/new/using.html
[2]: http://z.cs.utexas.edu/wiki/flame.wiki/FrontPage

